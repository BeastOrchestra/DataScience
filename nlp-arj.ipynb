{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport random\nimport re\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\n# Grab the data\n\ntrain = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-29T17:59:55.916024Z","iopub.execute_input":"2022-07-29T17:59:55.916439Z","iopub.status.idle":"2022-07-29T17:59:55.958730Z","shell.execute_reply.started":"2022-07-29T17:59:55.916406Z","shell.execute_reply":"2022-07-29T17:59:55.957541Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-08-02T20:03:45.239642Z","iopub.execute_input":"2022-08-02T20:03:45.240196Z","iopub.status.idle":"2022-08-02T20:03:51.594815Z","shell.execute_reply.started":"2022-08-02T20:03:45.240036Z","shell.execute_reply":"2022-08-02T20:03:51.593718Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Step 1: Background and Problem description\n\nFor Module 4 in MSDS 5511 - Deep learning & AI, we have been asked to use recurrent neural networks to build a tweet classifier. A kaggle competition will be used to demonstrate knowledge of RNNs relating to natural language processing models (NLPs henceforth).\n\nA dataset containing tweets with a binary incidator of whether or not the tweet is specifically relating to a natural disaster will be used to train a moedl.","metadata":{}},{"cell_type":"markdown","source":"## Step 2 - EDA\n\nExploratory data analysis will be performed on the data\n\n","metadata":{}},{"cell_type":"code","source":"print(train.head(5))\n\n# We will only concern ourselves with the text\n\ntrain = train.drop(['id','keyword','location'], axis=1)\ntest = test.drop(['id','keyword','location'], axis=1)\n\n# Counts of binary target data\nprint(train.target.value_counts())\n","metadata":{"execution":{"iopub.status.busy":"2022-07-29T17:59:57.799779Z","iopub.execute_input":"2022-07-29T17:59:57.800877Z","iopub.status.idle":"2022-07-29T17:59:57.813652Z","shell.execute_reply.started":"2022-07-29T17:59:57.800831Z","shell.execute_reply":"2022-07-29T17:59:57.812340Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"We'd prefer to have balanced data. This will involve sampling an equal amount of 0s vs. 1s in the training set. Let's see what the trainnig set looks like first","metadata":{}},{"cell_type":"code","source":"NumTweets = 20\n\nrandomTweetIndex =  random.sample(list(train.index),NumTweets)\n\nfor i in randomTweetIndex:\n    print(train.text[i])\n","metadata":{"execution":{"iopub.status.busy":"2022-07-29T17:59:59.527803Z","iopub.execute_input":"2022-07-29T17:59:59.528698Z","iopub.status.idle":"2022-07-29T17:59:59.535883Z","shell.execute_reply.started":"2022-07-29T17:59:59.528656Z","shell.execute_reply":"2022-07-29T17:59:59.534950Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"First, we can remove all redudant duplicate tweets","metadata":{}},{"cell_type":"code","source":"print('Before duplicates are removed: ',len(train))\ntrain = train.drop_duplicates(subset='text', keep=\"first\")\nprint('After Duplicates are removed: ',len(train))\n","metadata":{"execution":{"iopub.status.busy":"2022-07-29T18:00:01.184313Z","iopub.execute_input":"2022-07-29T18:00:01.185520Z","iopub.status.idle":"2022-07-29T18:00:01.196144Z","shell.execute_reply.started":"2022-07-29T18:00:01.185477Z","shell.execute_reply":"2022-07-29T18:00:01.194801Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"A couple of things that immediately stand out are \"@\" direct mentions and 'http://' links.\n\nLet's drop these two from the training group","metadata":{}},{"cell_type":"code","source":"# Note: functions below are derived from examples found at: https://docs.python.org/3/library/re.html\n\n\n# Remove http links and @ mentions\ndef drop_links(line):\n    link = re.compile(r'https?://\\S+')\n    return link.sub(r'', line)\n\ndef drop_mentions(line):\n    tgt_twt = re.compile(r'@\\S+')\n    return tgt_twt.sub(r'', line)\n\ndef dropJunk_data(data):\n    data['text'] = data['text'].apply(lambda x : drop_links(x))\n    data['text'] = data['text'].apply(lambda x : drop_mentions(x))    \n    return data","metadata":{"execution":{"iopub.status.busy":"2022-07-29T18:00:04.446024Z","iopub.execute_input":"2022-07-29T18:00:04.446640Z","iopub.status.idle":"2022-07-29T18:00:04.452719Z","shell.execute_reply.started":"2022-07-29T18:00:04.446606Z","shell.execute_reply":"2022-07-29T18:00:04.451754Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"wiped_train = dropJunk_data(train)\nwiped_test = dropJunk_data(test)\n\nprint('Training length :',len(wiped_train))\nprint(wiped_train['target'].value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-07-29T18:00:06.031497Z","iopub.execute_input":"2022-07-29T18:00:06.031897Z","iopub.status.idle":"2022-07-29T18:00:06.082547Z","shell.execute_reply.started":"2022-07-29T18:00:06.031862Z","shell.execute_reply":"2022-07-29T18:00:06.081507Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"The data is imbalanced. Downsample 0s to make it the same size as 1s","metadata":{}},{"cell_type":"code","source":"# Downsample 0s, concat 1s and zeros, shuffle\n\nzeros = wiped_train[wiped_train['target']==0]\nones = wiped_train[wiped_train['target']==1]\n\nzeros_sample = zeros.sample(n=len(ones))\n\nwiped_train = pd.concat([zeros_sample,ones]).sample(frac=1, random_state=42).reset_index(drop=True)\nwiped_train.head()\nwiped_train['target'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-07-29T18:00:07.784705Z","iopub.execute_input":"2022-07-29T18:00:07.785368Z","iopub.status.idle":"2022-07-29T18:00:07.805541Z","shell.execute_reply.started":"2022-07-29T18:00:07.785329Z","shell.execute_reply":"2022-07-29T18:00:07.804679Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# remove stop words\n# NOTE: I have looked at examples found at: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n\nfrom nltk.corpus import stopwords\n\ndef drop_stopwords(sentence):\n    sen = sentence.split()\n    wrd = [word for word in sen if word not in stopwords.words('english')]\n    \n    return ' '.join(wrd)\n\ndef processed_wo_stopwords(data):\n    data['text'] = data['text'].apply(lambda x : drop_stopwords(x))   \n    return data\n\ntrain = processed_wo_stopwords(wiped_train)\ntest = processed_wo_stopwords(wiped_test)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-29T18:00:09.861420Z","iopub.execute_input":"2022-07-29T18:00:09.862541Z","iopub.status.idle":"2022-07-29T18:00:28.531647Z","shell.execute_reply.started":"2022-07-29T18:00:09.862503Z","shell.execute_reply":"2022-07-29T18:00:28.530474Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Samples\nprint(train.head(10))","metadata":{"execution":{"iopub.status.busy":"2022-07-29T18:00:28.533628Z","iopub.execute_input":"2022-07-29T18:00:28.534050Z","iopub.status.idle":"2022-07-29T18:00:28.541791Z","shell.execute_reply.started":"2022-07-29T18:00:28.534017Z","shell.execute_reply":"2022-07-29T18:00:28.540540Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Vectorize data:\n\nI will create a corpus that contains train and test data by concatonating the two. I will then pass the corpus in to the vectorizer","metadata":{}},{"cell_type":"code","source":"# Note: This was a helpful notebook for understanding vectorization: \n#  https://www.kaggle.com/code/mattbast/rnn-and-nlp-detect-a-disaster-in-tweets/notebook#Encode-sentences\n\nimport tensorflow as tf\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\n\n# Concat\ncorpus = pd.concat([train['text'],test['text']])\n    \ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\n\nmaxLen = max(len(c.split()) for c in corpus)\nprint('maximum length: ', maxLen)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T18:00:28.543159Z","iopub.execute_input":"2022-07-29T18:00:28.543480Z","iopub.status.idle":"2022-07-29T18:00:33.842971Z","shell.execute_reply.started":"2022-07-29T18:00:28.543450Z","shell.execute_reply":"2022-07-29T18:00:33.841813Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"trainFeatures = train.iloc[:,0]\ntrainLabels = train.iloc[:,1]\ntestFeatures = test.iloc[:,0]\n\n\ntrainToken = tokenizer.texts_to_sequences(trainFeatures)\ntestToken = tokenizer.texts_to_sequences(testFeatures)\n\n# Pad with extra length follow tweet (if blank)\ntrainPadded = pad_sequences(trainToken, maxlen=maxLen, padding='post')\ntestPadded = pad_sequences(testToken, maxlen=maxLen, padding='post')\n\n# Make an array of train labels for training the model\ntrainLabels = np.array(trainLabels)","metadata":{"execution":{"iopub.status.busy":"2022-07-29T18:00:33.845053Z","iopub.execute_input":"2022-07-29T18:00:33.845626Z","iopub.status.idle":"2022-07-29T18:00:34.033153Z","shell.execute_reply.started":"2022-07-29T18:00:33.845593Z","shell.execute_reply":"2022-07-29T18:00:34.032070Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Step 3: Model Arch.\n\nLet's build a model\n\nLSTM is the way to go here, first because it is part of the assignment and secondly because we are working with sequence data. Keras has lots of lstm support. Bidirectional LSTM considers the sequence of training data in terms of front-to-back and back-to-front to find patterns. For my first model I will do 1 LSTM with 128 hidden layers. I need to first embed the data and then compress the LSTM output with a series of dense fully connected layers with tanh activations. Since the problem is a binary classification problem, I will use a single neuron output layer with sigmoid activation. The loss function will be Binary crossentropy.","metadata":{}},{"cell_type":"code","source":"import keras\nfrom keras.layers import LSTM\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Bidirectional, Dropout, BatchNormalization\nfrom keras import optimizers\n\n\nEmbeddedWords = len(tokenizer.word_index)+1\nunits = 144  # Just guessing \nhidden_units = 128\n\nmodel_v1 = Sequential()\nmodel_v1.add(Embedding(EmbeddedWords, units, input_length = maxLen))\nmodel_v1.add(Bidirectional(LSTM(hidden_units)))\nmodel_v1.add(Dense(256, activation='tanh'))\nmodel_v1.add(Dense(128, activation='tanh'))\nmodel_v1.add(Dense(64, activation='tanh'))\nmodel_v1.add(Dense(1, activation='sigmoid'))\n\nmodel_v1.summary()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-07-29T18:06:34.083326Z","iopub.execute_input":"2022-07-29T18:06:34.083714Z","iopub.status.idle":"2022-07-29T18:06:34.637110Z","shell.execute_reply.started":"2022-07-29T18:06:34.083681Z","shell.execute_reply":"2022-07-29T18:06:34.635835Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"\nmodel_v1.compile( loss=tf.keras.losses.BinaryCrossentropy(),\n    optimizer=tf.keras.optimizers.Adam(0.00001),\n    metrics=['accuracy', 'Precision', 'Recall'])\n\n\n# Call back on test data as trigger\ncb = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1)]\n\n\nhistory_v1 = model_v1.fit(trainPadded, trainLabels, epochs=100, validation_split=0.25, callbacks = cb)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-29T18:01:38.782351Z","iopub.execute_input":"2022-07-29T18:01:38.783333Z","iopub.status.idle":"2022-07-29T18:04:39.999874Z","shell.execute_reply.started":"2022-07-29T18:01:38.783290Z","shell.execute_reply":"2022-07-29T18:04:39.999024Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(1,4, figsize=(16, 8))\n\naxs[0].set_title('Loss')\naxs[0].plot(history_v1.history['loss'], label='train')\naxs[0].plot(history_v1.history['val_loss'], label='val')\naxs[0].legend()\n\naxs[1].set_title('Accuracy')\naxs[1].plot(history_v1.history['accuracy'], label='train')\naxs[1].plot(history_v1.history['val_accuracy'], label='val')\naxs[1].legend()\n\naxs[2].set_title('Precision')\naxs[2].plot(history_v1.history['precision'], label='train')\naxs[2].plot(history_v1.history['val_precision'], label='val')\naxs[2].legend()\n\naxs[3].set_title('Recall')\naxs[3].plot(history_v1.history['recall'], label='train')\naxs[3].plot(history_v1.history['val_recall'], label='val')\naxs[3].legend()\n","metadata":{"execution":{"iopub.status.busy":"2022-07-29T18:04:40.588241Z","iopub.execute_input":"2022-07-29T18:04:40.588569Z","iopub.status.idle":"2022-07-29T18:04:41.401069Z","shell.execute_reply.started":"2022-07-29T18:04:40.588538Z","shell.execute_reply":"2022-07-29T18:04:41.399971Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Analysis:\n\nLooking at the loss function vs. epoch, clearly the training is overfitting. I will subsequently tighten up the patience to 3 and change the stopping criteria to Accuracy because it seems like there is room for improvement on that metric.\n\nI am going to experiement with dropout & Batchnorm as levers for improving test loss.\n","metadata":{}},{"cell_type":"code","source":"\nmodel_v2 = Sequential()\nmodel_v2.add(Embedding(EmbeddedWords, units, input_length = maxLen))\nmodel_v2.add(Bidirectional(LSTM(hidden_units)))\nmodel_v2.add(Dropout(0.25))\nmodel_v2.add(BatchNormalization())\nmodel_v2.add(Dense(128, activation='tanh'))\nmodel_v2.add(Dropout(0.25))\nmodel_v2.add(BatchNormalization())\nmodel_v2.add(Dense(64, activation='tanh'))\nmodel_v2.add(Dropout(0.25))\nmodel_v2.add(BatchNormalization())\nmodel_v2.add(Dense(1, activation='sigmoid'))\n\nmodel_v2.summary()\n\nmodel_v2.compile( loss=tf.keras.losses.BinaryCrossentropy(),\n    optimizer=tf.keras.optimizers.Adam(0.00001),\n    metrics=['accuracy', 'Precision', 'Recall'])\n\n# Call back on test data as trigger\ncb = [tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, verbose=1)]\nhistory_v2 = model_v2.fit(trainPadded, trainLabels,epochs=50,validation_split=0.2, callbacks = cb)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-29T18:22:55.583046Z","iopub.execute_input":"2022-07-29T18:22:55.583812Z","iopub.status.idle":"2022-07-29T18:27:53.023183Z","shell.execute_reply.started":"2022-07-29T18:22:55.583774Z","shell.execute_reply":"2022-07-29T18:27:53.022385Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(1, 4, figsize=(16, 8))\n\naxs[0].set_title('Loss')\naxs[0].plot(history_v2.history['loss'], label='train')\naxs[0].plot(history_v2.history['val_loss'], label='val')\naxs[0].legend()\n\naxs[1].set_title('Accuracy')\naxs[1].plot(history_v2.history['accuracy'], label='train')\naxs[1].plot(history_v2.history['val_accuracy'], label='val')\naxs[1].legend()\n\naxs[2].set_title('Precision')\naxs[2].plot(history_v2.history['precision'], label='train')\naxs[2].plot(history_v2.history['val_precision'], label='val')\naxs[2].legend()\n\naxs[3].set_title('Recall')\naxs[3].plot(history_v2.history['recall'], label='train')\naxs[3].plot(history_v2.history['val_recall'], label='val')\naxs[3].legend()\n","metadata":{"execution":{"iopub.status.busy":"2022-07-29T18:32:30.111398Z","iopub.execute_input":"2022-07-29T18:32:30.111830Z","iopub.status.idle":"2022-07-29T18:32:30.685683Z","shell.execute_reply.started":"2022-07-29T18:32:30.111793Z","shell.execute_reply":"2022-07-29T18:32:30.684875Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"\nDropout and batch normalization really helped the model in terms of generalization. The loss function indicates overtraining, but the accuracy of the model is improving while loss is no longer improving. \n\nI will now swap the tanh function with sigmoid for the dense layers to see how this impacts the model.\n","metadata":{}},{"cell_type":"code","source":"model_v3 = Sequential()\nmodel_v3.add(Embedding(EmbeddedWords, units, input_length = maxLen))\nmodel_v3.add(Bidirectional(LSTM(hidden_units)))\nmodel_v3.add(Dropout(0.25))\nmodel_v3.add(BatchNormalization())\nmodel_v3.add(Dense(128, activation='sigmoid'))\nmodel_v3.add(Dropout(0.25))\nmodel_v3.add(BatchNormalization())\nmodel_v3.add(Dense(64, activation='sigmoid'))\nmodel_v3.add(Dropout(0.25))\nmodel_v3.add(BatchNormalization())\nmodel_v3.add(Dense(1, activation='sigmoid'))\n\nmodel_v3.summary()\n\n#%%\n\nmodel_v3.compile( loss=tf.keras.losses.BinaryCrossentropy(),\n    optimizer=tf.keras.optimizers.Adam(0.00001),\n    metrics=['accuracy', 'Precision', 'Recall'])\n\n#%%\n\nhistory_v3 = model_v3.fit(trainPadded, trainLabels,epochs=50,validation_split=0.2, callbacks = cb)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-29T18:32:47.284030Z","iopub.execute_input":"2022-07-29T18:32:47.284475Z","iopub.status.idle":"2022-07-29T18:37:18.466715Z","shell.execute_reply.started":"2022-07-29T18:32:47.284439Z","shell.execute_reply":"2022-07-29T18:37:18.465663Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(1, 4, figsize=(20, 5))\n\naxs[0].set_title('Loss')\naxs[0].plot(history_v3.history['loss'], label='train')\naxs[0].plot(history_v3.history['val_loss'], label='val')\naxs[0].legend()\n\naxs[1].set_title('Accuracy')\naxs[1].plot(history_v3.history['accuracy'], label='train')\naxs[1].plot(history_v3.history['val_accuracy'], label='val')\naxs[1].legend()\n\naxs[2].set_title('Precision')\naxs[2].plot(history_v3.history['precision'], label='train')\naxs[2].plot(history_v3.history['val_precision'], label='val')\naxs[2].legend()\n\naxs[3].set_title('Recall')\naxs[3].plot(history_v3.history['recall'], label='train')\naxs[3].plot(history_v3.history['val_recall'], label='val')\naxs[3].legend()","metadata":{"execution":{"iopub.status.busy":"2022-07-29T18:39:48.586268Z","iopub.execute_input":"2022-07-29T18:39:48.586669Z","iopub.status.idle":"2022-07-29T18:39:49.170738Z","shell.execute_reply.started":"2022-07-29T18:39:48.586636Z","shell.execute_reply":"2022-07-29T18:39:49.169677Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"Switching to dense activations of Sigmoid vs. tanh showed marginal improvement to the accuracy performance in the test set.\n\nFor my last experiement I will add 1 more layer of Bidirectional LSTM to see if this increases performance.\n","metadata":{}},{"cell_type":"code","source":"model_v4 = Sequential()\nmodel_v4.add(Embedding(EmbeddedWords, units, input_length = maxLen))\nmodel_v4.add(Bidirectional(LSTM(hidden_units,return_sequences=True)))\nmodel_v4.add(Dropout(0.25))\nmodel_v4.add(Bidirectional(LSTM(hidden_units)))\nmodel_v4.add(Dropout(0.25))\nmodel_v4.add(BatchNormalization())\nmodel_v4.add(Dense(128, activation='sigmoid'))\nmodel_v4.add(Dropout(0.25))\nmodel_v4.add(BatchNormalization())\nmodel_v4.add(Dense(64, activation='sigmoid'))\nmodel_v4.add(Dropout(0.25))\nmodel_v4.add(BatchNormalization())\nmodel_v4.add(Dense(1, activation='sigmoid'))\n\nmodel_v4.summary()\n\n#%%\n\nmodel_v4.compile( loss=tf.keras.losses.BinaryCrossentropy(),\n    optimizer=tf.keras.optimizers.Adam(0.00001),\n    metrics=['accuracy', 'Precision', 'Recall'])\n\n#%%\n\nhistory_v4 = model_v4.fit(trainPadded, trainLabels,epochs=50,validation_split=0.2, callbacks = cb)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-29T18:40:17.916365Z","iopub.execute_input":"2022-07-29T18:40:17.917171Z","iopub.status.idle":"2022-07-29T18:47:01.484809Z","shell.execute_reply.started":"2022-07-29T18:40:17.917107Z","shell.execute_reply":"2022-07-29T18:47:01.483811Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"\nfig, axs = plt.subplots(1, 4, figsize=(20, 5))\n\naxs[0].set_title('Loss')\naxs[0].plot(history_v4.history['loss'], label='train')\naxs[0].plot(history_v4.history['val_loss'], label='val')\naxs[0].legend()\n\naxs[1].set_title('Accuracy')\naxs[1].plot(history_v4.history['accuracy'], label='train')\naxs[1].plot(history_v4.history['val_accuracy'], label='val')\naxs[1].legend()\n\naxs[2].set_title('Precision')\naxs[2].plot(history_v4.history['precision'], label='train')\naxs[2].plot(history_v4.history['val_precision'], label='val')\naxs[2].legend()\n\naxs[3].set_title('Recall')\naxs[3].plot(history_v4.history['recall'], label='train')\naxs[3].plot(history_v4.history['val_recall'], label='val')\naxs[3].legend()\n","metadata":{"execution":{"iopub.status.busy":"2022-07-29T18:48:44.663980Z","iopub.execute_input":"2022-07-29T18:48:44.665299Z","iopub.status.idle":"2022-07-29T18:48:45.219196Z","shell.execute_reply.started":"2022-07-29T18:48:44.665241Z","shell.execute_reply":"2022-07-29T18:48:45.217946Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"An additional LSTM layer is not improving from the model_v3 performance. This will be the model to use going forward.","metadata":{}},{"cell_type":"markdown","source":"\n## Step 4: Results and Analysis\n\nMultiple models were used to predict tweet classification based on text data. A single Bidirectional LSTM with dense fully connected layers each containing a sigmoid activation and Dropout & batchnormalization had the best convergence on maximum accuracy. The best accuracy in test data for model_v3 was 75.16%. Different activations on the dense layers as well as adding an extra LSTM layer did not improve the model.\n\n","metadata":{}},{"cell_type":"code","source":"\npreds = model_v3.predict(testPadded)\n\nlen(preds)\n\npredictions = []\n\nfor pred in preds:\n    if pred >= 0.5:\n        predictions.append(1)\n    else:\n        predictions.append(0)\n        \nsubmission = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\nsubmission\n\nsubmission['target']=predictions\nsubmission\n\n\nsubmission.to_csv(\"submission.csv\", index=False)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-07-29T19:04:08.986796Z","iopub.execute_input":"2022-07-29T19:04:08.987200Z","iopub.status.idle":"2022-07-29T19:04:10.338515Z","shell.execute_reply.started":"2022-07-29T19:04:08.987164Z","shell.execute_reply":"2022-07-29T19:04:10.337658Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"\n# Conclusion\n\nI have only looked at LSTM layers and dense fully connected layers. I have seen other architecture like CNN used in conjunction w/ LSTM layers, but I did not study this for the problem being investigated. Different vectorization methods may yield better results. Obviously, more input features such as location and time may greatly enhance the model performance. Based on this notebook, it seems clear that putting more LSTM layers in to the model does not improve performance.\n","metadata":{}}]}